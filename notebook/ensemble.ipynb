{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# import packages\nimport collections\nimport os\nimport random\nimport json\n\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom torch.nn import CrossEntropyLoss\nfrom torch.utils.data import DataLoader, SequentialSampler\nfrom tqdm import tqdm\nimport pandas as pd\n\nfrom transformers import (\n    AutoTokenizer,\n    squad_convert_examples_to_features,\n    BasicTokenizer, ElectraModel, ELECTRA_PRETRAINED_MODEL_ARCHIVE_MAP)\nfrom transformers.data.metrics.squad_metrics import (\n    compute_predictions_logits,\n)\nfrom transformers.data.processors.squad import SquadResult, SquadV1Processor\nfrom transformers.modeling_electra import ElectraPreTrainedModel\nfrom collections import OrderedDict\nfrom transformers import (\n    AlbertConfig,\n    AutoConfig,\n    BertConfig,\n    DistilBertConfig,\n    ElectraConfig,\n    RobertaConfig,\n    PretrainedConfig,\n    AlbertForQuestionAnswering,\n    BertForQuestionAnswering,\n    DistilBertForQuestionAnswering,\n    RobertaForQuestionAnswering,\n)","execution_count":25,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# utils\nclass DottableDict(dict):\n    def __init__(self, *args, **kwargs):\n        dict.__init__(self, *args, **kwargs)\n        self.__dict__ = self\n    def allowDotting(self, state=True):\n        if state:\n            self.__dict__ = self\n        else:\n            self.__dict__ = dict()\n\ndef do_qa_test(test):\n    output = {}\n    output['version'] = 'v1.0'\n    output['data'] = []\n    for line in test:\n        paragraphs=[]\n        context = line[1]\n        qas = []\n        question = line[-1]\n        qid = line[0]\n        if type(context) != str or type(question) != str:\n            print(context, type(context))\n            print(question, type(question))\n            continue\n        answers = []\n        answers.append({'answer_start': 1000000, 'text': '__None__'})\n        qas.append({'question': question, 'id': qid, 'is_impossible': False, 'answers': answers})\n        paragraphs.append({'context': context, 'qas': qas})\n        output['data'].append({'title': 'None', 'paragraphs': paragraphs})\n    return output\n\ndef set_seed(args):\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n    torch.manual_seed(args.seed)\n\n\ndef to_list(tensor):\n    return tensor.detach().cpu().tolist()\n\ndef _get_best_indexes(logits, n_best_size):\n    \"\"\"Get the n-best logits from a list.\"\"\"\n    index_and_score = sorted(enumerate(logits), key=lambda x: x[1], reverse=True)\n\n    best_indexes = []\n    for i in range(len(index_and_score)):\n        if i >= n_best_size:\n            break\n        best_indexes.append(index_and_score[i][0])\n    return best_indexes\n\n\ndef get_final_text(pred_text, orig_text, do_lower_case):\n\n    def _strip_spaces(text):\n        ns_chars = []\n        ns_to_s_map = collections.OrderedDict()\n        for (i, c) in enumerate(text):\n            if c == \" \":\n                continue\n            ns_to_s_map[len(ns_chars)] = i\n            ns_chars.append(c)\n        ns_text = \"\".join(ns_chars)\n        return (ns_text, ns_to_s_map)\n\n    # We first tokenize `orig_text`, strip whitespace from the result\n    # and `pred_text`, and check if they are the same length. If they are\n    # NOT the same length, the heuristic has failed. If they are the same\n    # length, we assume the characters are one-to-one aligned.\n    tokenizer = BasicTokenizer(do_lower_case=do_lower_case)\n\n    tok_text = \" \".join(tokenizer.tokenize(orig_text))\n\n    start_position = tok_text.find(pred_text)\n    if start_position == -1:\n        return orig_text\n    end_position = start_position + len(pred_text) - 1\n\n    (orig_ns_text, orig_ns_to_s_map) = _strip_spaces(orig_text)\n    (tok_ns_text, tok_ns_to_s_map) = _strip_spaces(tok_text)\n\n    if len(orig_ns_text) != len(tok_ns_text):\n        return orig_text\n\n    # We then project the characters in `pred_text` back to `orig_text` using\n    # the character-to-character alignment.\n    tok_s_to_ns_map = {}\n    for (i, tok_index) in tok_ns_to_s_map.items():\n        tok_s_to_ns_map[tok_index] = i\n\n    orig_start_position = None\n    if start_position in tok_s_to_ns_map:\n        ns_start_position = tok_s_to_ns_map[start_position]\n        if ns_start_position in orig_ns_to_s_map:\n            orig_start_position = orig_ns_to_s_map[ns_start_position]\n\n    if orig_start_position is None:\n        return orig_text\n\n    orig_end_position = None\n    if end_position in tok_s_to_ns_map:\n        ns_end_position = tok_s_to_ns_map[end_position]\n        if ns_end_position in orig_ns_to_s_map:\n            orig_end_position = orig_ns_to_s_map[ns_end_position]\n\n    if orig_end_position is None:\n        return orig_text\n\n    output_text = orig_text[orig_start_position : (orig_end_position + 1)]\n    return output_text\n\n","execution_count":26,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class ElectraForQuestionAnswering(ElectraPreTrainedModel):\n    \"\"\"\n    Identical to BertForQuestionAnswering other than using an ElectraModel\n    \"\"\"\n\n    config_class = ElectraConfig\n    pretrained_model_archive_map = ELECTRA_PRETRAINED_MODEL_ARCHIVE_MAP\n    base_model_prefix = \"electra\"\n\n    def __init__(self, config, weight=None):\n        config.output_hidden_states = True\n        super().__init__(config)\n        self.num_labels = config.num_labels\n        self.combine_hidden = args.combine_hidden\n        self.electra = ElectraModel(config)\n        self.drop_out = nn.Dropout(args.drop_out)\n        if args.combine_hidden:\n            self.qa_outputs = nn.Linear(config.hidden_size * 2, config.num_labels)\n        else:\n            self.qa_outputs = nn.Linear(config.hidden_size, config.num_labels)\n\n        self.init_weights()\n\n    def forward(\n            self,\n            input_ids=None,\n            attention_mask=None,\n            token_type_ids=None,\n            position_ids=None,\n            head_mask=None,\n            inputs_embeds=None,\n            start_positions=None,\n            end_positions=None,\n    ):\n\n        outputs = self.electra(input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds)\n\n        if self.combine_hidden:\n            all_hidden_states = outputs[1]\n            sequence_output = torch.cat((all_hidden_states[-1], all_hidden_states[-2]), dim=-1)\n        else:\n            sequence_output = outputs[0]\n        sequence_output = self.drop_out(sequence_output)\n        logits = self.qa_outputs(sequence_output)\n\n        start_logits, end_logits = logits.split(1, dim=-1)\n        start_logits = start_logits.squeeze(-1)\n        end_logits = end_logits.squeeze(-1)\n\n        outputs = (start_logits, end_logits,) + outputs[2:]\n        if start_positions is not None and end_positions is not None:\n            # If we are on multi-GPU, split add a dimension\n            if len(start_positions.size()) > 1:\n                start_positions = start_positions.squeeze(-1)\n            if len(end_positions.size()) > 1:\n                end_positions = end_positions.squeeze(-1)\n            # sometimes the start/end positions are outside our model inputs, we ignore these terms\n            ignored_index = start_logits.size(1)\n            start_positions.clamp_(0, ignored_index)\n            end_positions.clamp_(0, ignored_index)\n\n            loss_fct = CrossEntropyLoss(ignore_index=ignored_index)\n            start_loss = loss_fct(start_logits, start_positions)\n            end_loss = loss_fct(end_logits, end_positions)\n            total_loss = (start_loss + end_loss) / 2\n            outputs = (total_loss,) + outputs\n\n        return outputs  # (loss), start_logits, end_logits, (hidden_states), (attentions)\n","execution_count":27,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"MODEL_FOR_QUESTION_ANSWERING_MAPPING = OrderedDict(\n    [\n        (DistilBertConfig, DistilBertForQuestionAnswering),\n        (AlbertConfig, AlbertForQuestionAnswering),\n        (RobertaConfig, RobertaForQuestionAnswering),\n        (BertConfig, BertForQuestionAnswering),\n        (ElectraConfig, ElectraForQuestionAnswering),\n    ]\n)\n\nclass AutoModelForQuestionAnswering(object):\n\n    def __init__(self):\n        raise EnvironmentError(\n            \"AutoModelForQuestionAnswering is designed to be instantiated \"\n            \"using the `AutoModelForQuestionAnswering.from_pretrained(pretrained_model_name_or_path)` or \"\n            \"`AutoModelForQuestionAnswering.from_config(config)` methods.\"\n        )\n\n    @classmethod\n    def from_config(cls, config):\n        for config_class, model_class in MODEL_FOR_QUESTION_ANSWERING_MAPPING.items():\n            if isinstance(config, config_class):\n                return model_class(config)\n\n        raise ValueError(\n            \"Unrecognized configuration class {} for this kind of AutoModel: {}.\\n\"\n            \"Model type should be one of {}.\".format(\n                config.__class__,\n                cls.__name__,\n                \", \".join(c.__name__ for c in MODEL_FOR_QUESTION_ANSWERING_MAPPING.keys()),\n            )\n        )\n\n    @classmethod\n    def from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs):\n        config = kwargs.pop(\"config\", None)\n        if not isinstance(config, PretrainedConfig):\n            config = AutoConfig.from_pretrained(pretrained_model_name_or_path, **kwargs)\n\n        for config_class, model_class in MODEL_FOR_QUESTION_ANSWERING_MAPPING.items():\n            if isinstance(config, config_class):\n                return model_class.from_pretrained(pretrained_model_name_or_path, *model_args, config=config, **kwargs)\n\n        raise ValueError(\n            \"Unrecognized configuration class {} for this kind of AutoModel: {}.\\n\"\n            \"Model type should be one of {}.\".format(\n                config.__class__,\n                cls.__name__,\n                \", \".join(c.__name__ for c in MODEL_FOR_QUESTION_ANSWERING_MAPPING.keys()),\n            )\n        )\n","execution_count":28,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_and_cache_examples(args, tokenizer, evaluate=False, output_examples=False):\n\n\n    processor = SquadV1Processor()\n    examples = processor.get_dev_examples(\".\", filename=args.predict_file)\n\n    features, dataset = squad_convert_examples_to_features(\n        examples=examples,\n        tokenizer=tokenizer,\n        max_seq_length=args.max_seq_length,\n        doc_stride=args.doc_stride,\n        max_query_length=args.max_query_length,\n        is_training=not evaluate,\n        return_dataset=\"pt\",\n        threads=1,\n    )\n\n    if output_examples:\n        return dataset, examples, features\n    return dataset\n\ndef compute_predictions_logits(\n    all_examples,\n    all_features,\n    all_results,\n    n_best_size,\n    max_answer_length,\n    do_lower_case,\n    tokenizer,\n):\n\n    example_index_to_features = collections.defaultdict(list)\n    for feature in all_features:\n        example_index_to_features[feature.example_index].append(feature)\n\n    unique_id_to_result = {}\n    for result in all_results:\n        unique_id_to_result[result.unique_id] = result\n\n    _PrelimPrediction = collections.namedtuple(  # pylint: disable=invalid-name\n        \"PrelimPrediction\", [\"feature_index\", \"start_index\", \"end_index\", \"start_logit\", \"end_logit\"]\n    )\n\n    all_predictions = collections.OrderedDict()\n\n    for (example_index, example) in enumerate(all_examples):\n        features = example_index_to_features[example_index]\n\n        prelim_predictions = []\n        for (feature_index, feature) in enumerate(features):\n            result = unique_id_to_result[feature.unique_id]\n            start_indexes = _get_best_indexes(result.start_logits, n_best_size)\n            end_indexes = _get_best_indexes(result.end_logits, n_best_size)\n            for start_index in start_indexes:\n                for end_index in end_indexes:\n                    # We could hypothetically create invalid predictions, e.g., predict\n                    # that the start of the span is in the question. We throw out all\n                    # invalid predictions.\n                    if start_index >= len(feature.tokens):\n                        continue\n                    if end_index >= len(feature.tokens):\n                        continue\n                    if start_index not in feature.token_to_orig_map:\n                        continue\n                    if end_index not in feature.token_to_orig_map:\n                        continue\n                    if not feature.token_is_max_context.get(start_index, False):\n                        continue\n                    if end_index < start_index:\n                        continue\n                    length = end_index - start_index + 1\n                    if length > max_answer_length:\n                        continue\n                    prelim_predictions.append(\n                        _PrelimPrediction(\n                            feature_index=feature_index,\n                            start_index=start_index,\n                            end_index=end_index,\n                            start_logit=result.start_logits[start_index],\n                            end_logit=result.end_logits[end_index],\n                        )\n                    )\n        prelim_predictions = sorted(prelim_predictions, key=lambda x: (x.start_logit + x.end_logit), reverse=True)\n\n        _NbestPrediction = collections.namedtuple(  # pylint: disable=invalid-name\n            \"NbestPrediction\", [\"text\", \"start_logit\", \"end_logit\"]\n        )\n\n        seen_predictions = {}\n        nbest = []\n        for pred in prelim_predictions:\n            if len(nbest) >= n_best_size:\n                break\n            feature = features[pred.feature_index]\n            if pred.start_index > 0:  # this is a non-null prediction\n                tok_tokens = feature.tokens[pred.start_index : (pred.end_index + 1)]\n                orig_doc_start = feature.token_to_orig_map[pred.start_index]\n                orig_doc_end = feature.token_to_orig_map[pred.end_index]\n                orig_tokens = example.doc_tokens[orig_doc_start : (orig_doc_end + 1)]\n\n                tok_text = tokenizer.convert_tokens_to_string(tok_tokens)\n\n                # Clean whitespace\n                tok_text = tok_text.strip()\n                tok_text = \" \".join(tok_text.split())\n                orig_text = \" \".join(orig_tokens)\n\n                final_text = get_final_text(tok_text, orig_text, do_lower_case)\n                if final_text in seen_predictions:\n                    continue\n\n                seen_predictions[final_text] = True\n            else:\n                final_text = \"\"\n                seen_predictions[final_text] = True\n\n            nbest.append(_NbestPrediction(text=final_text, start_logit=pred.start_logit, end_logit=pred.end_logit))\n\n        nbest_json = []\n        for (i, entry) in enumerate(nbest):\n            output = collections.OrderedDict()\n            output[\"text\"] = entry.text\n            nbest_json.append(output)\n            \n        if not nbest:\n            nbest.append(_NbestPrediction(text=\"empty\", start_logit=0.0, end_logit=0.0))\n\n        all_predictions[example.qas_id] = nbest_json[0][\"text\"]\n\n    predictions_df = pd.DataFrame(list(all_predictions.items()), columns=['textID', 'answer'])\n    return predictions_df\n","execution_count":29,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\ndef evaluate(args, model, tokenizer, prefix=\"\"):\n    dataset, examples, features = load_and_cache_examples(args, tokenizer, evaluate=True, output_examples=True)\n\n    # Note that DistributedSampler samples randomly\n    eval_sampler = SequentialSampler(dataset)\n    eval_dataloader = DataLoader(dataset, sampler=eval_sampler, batch_size=args.eval_batch_size)\n\n    all_results = []\n\n    for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n        batch = tuple(t.to(args.device) for t in batch)\n\n        with torch.no_grad():\n            inputs = {\n                \"input_ids\": batch[0],\n                \"attention_mask\": batch[1],\n                \"token_type_ids\": batch[2],\n            }\n\n            if args.model_type in [\"xlm\", \"roberta\", \"distilbert\", \"camembert\"]:\n                del inputs[\"token_type_ids\"]\n\n            feature_indices = batch[3]\n            outputs=[]\n            for i,_model in enumerate(model):\n                outputs.append(_model(**inputs))\n        \n        for i, feature_index in enumerate(feature_indices):\n            eval_feature = features[feature_index.item()]\n            unique_id = int(eval_feature.unique_id)\n            output=[None]*len(outputs)\n            start_logits=[None]*len(outputs)\n            end_logits=[None]*len(outputs)\n            for j, _outputs in enumerate(outputs):\n                output[j] = [to_list(o[i]) for o in _outputs]\n                start_logits[j], end_logits[j] = output[j]\n\n            s_logits = np.mean(start_logits, axis=0)\n            e_logits = np.mean(end_logits, axis=0)\n\n            result = SquadResult(unique_id, s_logits, e_logits)\n\n            all_results.append(result)\n\n\n    predictions_df = compute_predictions_logits(\n        examples,\n        features,\n        all_results,\n        args.n_best_size,\n        args.max_answer_length,\n        args.do_lower_case,\n        tokenizer,\n    )\n\n    sub_df['selected_text'] = predictions_df['answer']\n    if post_process:\n        sub_df.loc[test_df['sentiment'] == 'neutral', 'selected_text'] = test_df['text']\n    sub_df.to_csv('submission.csv', index=False)\n    print('file submitted!')","execution_count":30,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!mkdir -p data\ntest_df = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/test.csv')\nsub_df = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/sample_submission.csv')\n\ntest = np.array(test_df)\nqa_test = do_qa_test(test)\nwith open('data/test.json', 'w') as outfile:\n    json.dump(qa_test, outfile)\n\nargs = DottableDict()\nargs.do_lower_case = True\nargs.overwrite_cache = True\nargs.seed = 42\nargs.model_type = 'electra'\nargs.model_name_or_path = ['/kaggle/input/out12epoch4','/kaggle/input/out12epoch2']\nargs.drop_out = 0.3\nargs.combine_hidden = True\nargs.eval_batch_size = 32\nargs.n_best_size = 20\nargs.max_seq_length = 192\nargs.max_query_length = 20\nargs.max_answer_length = 192\nargs.doc_stride = 64\nargs.predict_file = 'data/test.json'\npost_process=True\n\n\ndef run_squad(split=0):\n    args.split = split\n\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    args.device = device\n\n    # Set seed\n    set_seed(args)\n\n    args.model_type = args.model_type.lower()\n \n    tokenizer = AutoTokenizer.from_pretrained(\n        args.model_name_or_path[0],\n        do_lower_case=args.do_lower_case,\n        cache_dir=None,\n    )\n    model = []\n    for i, path in enumerate(args.model_name_or_path):\n        model.append(AutoModelForQuestionAnswering.from_pretrained(path))\n        model[i].to(args.device)\n        model[i].eval()\n\n    # Evaluate\n    evaluate(args, model, tokenizer, prefix=split)","execution_count":31,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"run_squad()","execution_count":32,"outputs":[{"output_type":"stream","text":"100%|██████████| 3534/3534 [00:00<00:00, 14342.46it/s]\nconvert squad examples to features: 100%|██████████| 3534/3534 [00:05<00:00, 616.16it/s]\nadd example index and unique id: 100%|██████████| 3534/3534 [00:00<00:00, 564007.09it/s]\nEvaluating:   1%|          | 1/111 [00:00<00:46,  2.38it/s]","name":"stderr"},{"output_type":"stream","text":"2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n","name":"stdout"},{"output_type":"stream","text":"\rEvaluating:   2%|▏         | 2/111 [00:00<00:45,  2.41it/s]","name":"stderr"},{"output_type":"stream","text":"2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n","name":"stdout"},{"output_type":"stream","text":"\rEvaluating:   3%|▎         | 3/111 [00:01<00:44,  2.43it/s]","name":"stderr"},{"output_type":"stream","text":"2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n","name":"stdout"},{"output_type":"stream","text":"\rEvaluating:   4%|▎         | 4/111 [00:01<00:43,  2.44it/s]","name":"stderr"},{"output_type":"stream","text":"2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n","name":"stdout"},{"output_type":"stream","text":"\rEvaluating:   5%|▍         | 5/111 [00:02<00:43,  2.46it/s]","name":"stderr"},{"output_type":"stream","text":"2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n","name":"stdout"},{"output_type":"stream","text":"\rEvaluating:   5%|▌         | 6/111 [00:02<00:42,  2.47it/s]","name":"stderr"},{"output_type":"stream","text":"2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n","name":"stdout"},{"output_type":"stream","text":"\rEvaluating:   6%|▋         | 7/111 [00:02<00:42,  2.47it/s]","name":"stderr"},{"output_type":"stream","text":"2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n","name":"stdout"},{"output_type":"stream","text":"Evaluating:   6%|▋         | 7/111 [00:03<00:47,  2.18it/s]\n","name":"stderr"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-32-051afc518134>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrun_squad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-31-08f697ac569d>\u001b[0m in \u001b[0;36mrun_squad\u001b[0;34m(split)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;31m# Evaluate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-30-6bf5a54dfab0>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(args, model, tokenizer, prefix)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_index\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m             \u001b[0meval_feature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m             \u001b[0munique_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_feature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}